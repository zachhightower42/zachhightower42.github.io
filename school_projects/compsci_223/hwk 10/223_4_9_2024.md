# Instruction Pipelining
- Divide the execution of an instruction into a series of sequential steps, stages, performed by different hardware units
- Here, check the visualization, stage 3 example 
- Note the overlap in the 3 stage pipeline, C, B, and A are running at the same time at the red line
- We complete the execution of the 3 instructions much faster using the pipeline 
- Without the pipeline, we would do it sequentially, which means we will take the unpipelined time

# Real world pipelines, car washes

Compare doing a car wash step by step yourself, versus running the car through a car wash, where each step is separated out and made efficient. 

When generating machine code the 

# Instruc. Pipelinining
![[Pasted image 20240409094637.png]]
- 3 stage pipeline
- We can increase the clock frequency to increase the clock frequency we introduce the smaller stages, the more stages we have, the better it is for performance
- Higher the number of stages, the better
- Execute multiple instructions with a different part of the instructions executed in parallel

# Example instructions executions steps (5 stages)
Stage 1: Fetch
Read (fetch) the instruction from memory 
Stage 2 Decode 
-Understand instructions and registers
Stage 3 Execute
- Compute value or address (e.g. add, subtract, multiply, move, etc.)
Stage 4 Memory
- Read of write data from/to memory
- This is an example of RISC machine which has dedicated memory load and store
Stage 5 Write Back
- Write results to registers

# Unpipelined vs Pipelined examples
- The long period will be the saturated (full) portion in the middle, drawn over by the red line

# Limitations, nonuniform delays
![[Pasted image 20240409094554.png]]
- Throughput is limited by the slowest stage (stage B in the example)
- Other stages sit idle and wait , stage A has to wait the longest
- Challenging to partition instructions and execution into perfectly balanced stages 

# Pipeline speedup
- If all stages are perfectly balanced (i.e. all stages take the same time)
$$Execution~Time_{pipelined}=\frac{Execution~Time_{nonpipelined}}{number of stages}+ time~for~filling~and~exiting $$
- Speed up maximum = number of stages
- Recent hardware uses about 40 stages 
- If not balanced, the speedup is less
- Speedup due to increased throughput
	- Execution time of each instruction does not decrease but increase slightly due to stage imbalance and increased hardware complexity
- The actual execution speed gets worse slightly, however due to the increased overall throughput, we get more accomplished in the same time
# Hazards
- Situations that prevent starting the next instruction in the next cycle
- Three types of hazard
	- Structural hazard
		- A required source does not exist (or is busy)
	- Data hazard
		- Need to wait for previous instruction to complete the data read/write
	- Control hazard
		- Deciding on control-flow action (e.g conditional jump) depends on previous instruction
# Structural hazard
- Conflict for use (or lack) of a hardware resource
- Example: memory access
	- Memory stage and instruction Fetch stage can't be executed at the same time if memory subsystem does not support two simultaneous accesses
		- This causes a pipeline bubble (**hardware stall**)
- Hence, pipelining require separate instruction / data memories or separate instruction / data caches
The L1 cache for instruction and the L2 cache for data represents an attempt to solve this problem 

# Data hazards 
- An instruction depends on completion of data by a previous instruction
- `add $t0, $t1, $s0$`
  `sub $s0, $t3, $t2$`
So one instruction depends on the completion of the previous instruction
This creates a dependent relationship, where we must 

The general idea for how to solve this, is to look for an independent instruction somewhere. 
If we put that independent instruction somewhere, we can use the time that would be taken up by the bubble. 

In that way, we could completely fill the bubble caused by the instruction

Compilers usually do this, they shuffle the order of instructions to make the program the most efficient that it can be



![[Pasted image 20240409100554.png]]
# Additional fix for data hazard, Forwarding (bypassing)

- Use a result as soon as it is computed
	- Don't wait for it to be stored in a register
	- Requires extra connections in the hardware circuit
This means that as soon as we add, we have the results, and then we allow the next instruction to use those results. We overload one thing so that it can be both output for one instruction and input for the next instruction

This technique is used everywhere, it doesn't solve every data issue, but it does solve it for simple issues like the one presented

![[Pasted image 20240409101131.png]]

# Data dependency
- 4 combinations with respect to read and write
	- RAW (Read After Write) - true data dependence
	- WAW (Write after write) - output dependence 
	- WAR (Write after Read) - anti dependence
	- RAR (Read after read) - not a dependence
- WAW and WAR are name dependencies and can be fixed by renaming
technique.
- Name dependencies occur when two instructions use the same register
or memory location.

# Example of WAW
`add t0, t1, t2`
`sub t2, t3, t0 `
`MUL t1, t5, t0 `
If we write and then read, we would go from t2 to t2
This is true data dependency. We must follow that order, there is no way to get around it. 

We also have an example of write after write. sub and MUL instruc. both write to t0 at the end

We can actually solve WAW / output dependency
Technique is called Register Renaming
To do this, instead of t0, the compiler may use t9 as the storage for its final value. Thus eliminating output dependency

WAR 
We have an example, at the beginning, we read from t0, and then we write to it in the next instruction 
Register renaming also resolves this issue and allows us to achieve independence 

RAR (read after read) Not an issue, it isn't dependent 

# Control hazards
- Branch determines the flow of instruction execution
	- Fetching next instruction depends on branch outcome
		- For example, do we fetch the first instruction of if or else body? it depends on the branch outcome
	- Pipeline can't always fetch correct instruction
		- Still working on ID stage of branch 
# Solution, Branch Prediction
- Longer pipelines can't readily determine branch outcome early
	-  Control hazard stall penalty becomes unacceptable
- Predict outcome of branch
	- Only stall if prediction is wrong
- Say we have a loop, we can predict that the loop will continue, and that will be true most of the time, so we can assume that, and only pay the cost of being wrong at the end of the loop 
- Real hardware uses very sophisticated techniques, it's been heavily researched
- Intel and AMD have invested massive amounts of money in finding better and better solutions for it
# Real world branch prediction 
- Static branch prediction 
	- Based on typical branch behavior 
	- Example: Loop branches
		- Predict backward branches taken (higher probability)
- Dynamic branch prediction 
	- Hardware measures actual branch behavior 
		- e.g Record recent history of each branch
	- Assume future behavior will continue along the trend 
		- When wrong, stall while re-fetching and updating the history
	- 
# Notes on clock freq
- We eventually got to a level where with power word and hit word, we could not further increase the performance benefit. The only thing we'd managed to increase was the power consumption.
- We instead looked to multi-core systems. The different core targets different clock frequency and different instructions to separate them out a little better and achieve a better performance